{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import argparse\n",
    "import utils\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from glob import glob\n",
    "# import apex.amp as amp\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Any, Optional, Tuple\n",
    "import multiprocessing\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIN_SAMPLES_PER_CLASS = 25\n",
    "\n",
    "# 36, 82, 34, 74\n",
    "SELECT_CATEGORIES = [36, 74, 82]\n",
    "IMAGES_DIR = \"/home/daniel/projects/transfer_learning/sample_files\"\n",
    "NUMBER_OF_TRAIN_IMAGES_PER_CLASS =25\n",
    "ORIGINAL_IMAGE_SIZE = 299\n",
    "IMAGE_SIZE = 299\n",
    "BATCH_SIZE = 256\n",
    "NUM_WORKERS = int(multiprocessing.cpu_count() / 2)\n",
    "BATCH_SIZE = 25\n",
    "RESNET_SIZE = 18\n",
    "PREDICT_ONLY = False\n",
    "LEARNING_RATE = 1e-3\n",
    "LR_STEP = 3\n",
    "LR_FACTOR = 0.5\n",
    "USE_PARALLEL = False\n",
    "NUM_EPOCHS = 40\n",
    "LOG_FREQ = 500\n",
    "NUM_TOP_PREDICTS = 1\n",
    "MAX_STEPS_PER_EPOCH = 2 ** 32\n",
    "PREDICT_ONLY = False\n",
    "TIME_LIMIT = 500 * 60 * 60\n",
    "IN_KERNEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_time_run_out() -> bool:\n",
    "    return time.time() - global_start_time > TIME_LIMIT - 500\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    ''' Computes and stores the average and current value '''\n",
    "    def __init__(self) -> None:\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.val = 0.0\n",
    "        self.avg = 0.0\n",
    "        self.sum = 0.0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val: float, n: int = 1) -> None:\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "        \n",
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, mode: str) -> None:\n",
    "        print(f'creating data loader - {mode}')\n",
    "        assert mode in ['train', 'val', 'test']\n",
    "\n",
    "        self.df = dataframe\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode == \"test\":\n",
    "            transforms_list = [\n",
    "                transforms.CenterCrop(ORIGINAL_IMAGE_SIZE),\n",
    "                transforms.Resize(IMAGE_SIZE)\n",
    "            ]\n",
    "        elif self.mode == \"train\":\n",
    "            transforms_list = [\n",
    "                transforms.CenterCrop(ORIGINAL_IMAGE_SIZE),\n",
    "                transforms.Resize(IMAGE_SIZE)\n",
    "                \n",
    "#                 transforms.Resize(ORIGINAL_IMAGE_SIZE)\n",
    "            ]\n",
    "        else:\n",
    "            transforms_list = [\n",
    "                transforms.CenterCrop(ORIGINAL_IMAGE_SIZE),\n",
    "                transforms.Resize(IMAGE_SIZE)\n",
    "            ]\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            transforms_list.extend([\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomResizedCrop(IMAGE_SIZE),\n",
    "                transforms.RandomChoice([\n",
    "                    transforms.RandomChoice([\n",
    "                        transforms.RandomResizedCrop(IMAGE_SIZE),\n",
    "                        transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "                        transforms.RandomAffine(degrees=15, translate=(0.2, 0.2),\n",
    "                                                scale=(0.8, 1.2), shear=15,\n",
    "                                                resample=Image.BILINEAR)\n",
    "                    ]),\n",
    "                    transforms.RandomChoice([\n",
    "                        transforms.Grayscale(num_output_channels=3),\n",
    "                        transforms.RandomRotation(degrees=90),\n",
    "                        transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n",
    "                        transforms.RandomAffine(degrees=15, translate=(0.2, 0.2),\n",
    "                                                scale=(0.8, 1.2), shear=15,\n",
    "                                                resample=Image.BILINEAR)\n",
    "                    ])\n",
    "                ])\n",
    "            ])\n",
    "\n",
    "\n",
    "        transforms_list.extend([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                  std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        self.transforms = transforms.Compose(transforms_list)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Any:\n",
    "        ''' Returns: tuple (sample, target) '''\n",
    "#         filename = self.df.image_id.values[index]\n",
    "        filename = \"COCO_train2014_{}.jpg\".format(str(self.df.image_id.values[index]).zfill(12))\n",
    "\n",
    "        part = 1 if self.mode == 'test' or filename[0] in '01234567' else 2\n",
    "#         directory = 'test' if self.mode == 'test' else 'train_' + filename[0]\n",
    "        # sample = Image.open(f'../input/google-landmarks-2019-64x64-part{part}/{directory}/{self.mode}_64/{filename}.jpg')\n",
    "        if self.mode == \"train\":\n",
    "            sample = Image.open(f'{IMAGES_DIR}/{filename}')\n",
    "        elif self.mode == \"val\":\n",
    "            sample = Image.open(f'{IMAGES_DIR}/{filename}')\n",
    "        else:\n",
    "            sample = Image.open(f'{IMAGES_DIR}/{filename}')\n",
    "        assert sample.mode == 'RGB'\n",
    "\n",
    "        image = self.transforms(sample)\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return image\n",
    "        else:\n",
    "            return image, self.df[\"category_id\"].values[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(checkpoint: any = None) -> 'Tuple[DataLoader[np.ndarray], DataLoader[np.ndarray], LabelEncoder, int]':\n",
    "    label_column = \"category_id\"\n",
    "    torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # only use classes which have at least MIN_SAMPLES_PER_CLASS samples\n",
    "    print('loading data...')\n",
    "    df = pd.read_csv(\"sample_images_labels.txt\")\n",
    "    image_files = [\"COCO_train2014_{}.jpg\".format(str(x).zfill(12)) for x in df[\"image_id\"].tolist()]\n",
    "\n",
    "    counts = df[label_column].value_counts()\n",
    "    selected_classes = counts[counts >= MIN_SAMPLES_PER_CLASS].index\n",
    "    num_classes = selected_classes.shape[0]\n",
    "    print('classes with at least N samples:', num_classes)\n",
    "    train_df = df.loc[df[\"category_id\"].isin(selected_classes)].copy()\n",
    "    print('train_df', train_df.shape)\n",
    "    train_exists = lambda img: os.path.exists(f'{IMAGES_DIR}/COCO_train2014_{str(img).zfill(12)}.jpg')\n",
    "    train_df = train_df.loc[train_df[\"image_id\"].apply(train_exists)].copy()\n",
    "    print('train_df after filtering', train_df.shape)\n",
    "    train_df = train_df.loc[df[label_column].isin(SELECT_CATEGORIES)].copy()\n",
    "    print(\"Train shape after filtering classes: \", train_df.shape)\n",
    "    new_counts = train_df[label_column].value_counts()\n",
    "    num_classes = new_counts.shape[0]\n",
    "\n",
    "\n",
    "    if checkpoint != None:\n",
    "        print(\"Loading label encoder from checkpoint...\")\n",
    "        label_encoder = checkpoint[\"label_encoder\"]\n",
    "    else:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder.fit(train_df.category_id.values)\n",
    "\n",
    "    y = train_df.pop(label_column)\n",
    "    x = train_df\n",
    "\n",
    "    train_size = NUMBER_OF_TRAIN_IMAGES_PER_CLASS * y.nunique()\n",
    "    train_x, val_x, train_y, val_y = train_test_split(x, y, train_size=train_size, random_state=42, stratify=y)\n",
    "    train_x[label_column] = train_y\n",
    "    val_x[label_column] = val_y\n",
    "\n",
    "    train_df = train_x\n",
    "    val_df = val_x\n",
    "    \n",
    "    y = val_df.pop(label_column)\n",
    "    x = val_df\n",
    "    val_x, test_x, val_y, test_y = train_test_split(x, y, test_size=0.5, random_state=42, stratify=y)\n",
    "    val_x[label_column] = val_y\n",
    "    test_x[label_column] = test_y\n",
    "\n",
    "    val_df = val_x\n",
    "    test_df = test_x\n",
    "\n",
    "    print(f\"Train length: {len(train_df)} Val length: {len(val_df)} Test length: {len(test_df)}\")\n",
    "\n",
    "#     test_df = pd.read_csv(csv_dir + 'test2.csv', dtype=str)\n",
    "#     print('test_df', test_df.shape)\n",
    "\n",
    "#     # filter non-existing test images\n",
    "#     exists = lambda img: os.path.exists(f'/hdd/kaggle/landmarks/test_images2/{img}.jpg')\n",
    "\n",
    "#     test_df = test_df.loc[test_df.id.apply(exists)].copy()\n",
    "#     print('test_df after filtering', test_df.shape)\n",
    "#     assert test_df.shape[0] > 112000\n",
    "#     # assert test_df.shape[0] > 117703\n",
    "    if PREDICT_ONLY:\n",
    "        num_classes = len(label_encoder.classes_)\n",
    "    print('found classes', len(label_encoder.classes_))\n",
    "    assert len(label_encoder.classes_) == num_classes\n",
    "\n",
    "    train_df[label_column] = label_encoder.transform(train_df[label_column])\n",
    "    val_df[label_column] = label_encoder.transform(val_df[label_column])\n",
    "    test_df[label_column] = label_encoder.transform(test_df[label_column])\n",
    "\n",
    "    train_dataset = ImageDataset(train_df, mode='train')\n",
    "    val_dataset = ImageDataset(val_df, mode='val')\n",
    "    test_dataset = ImageDataset(test_df, mode='test')\n",
    "    \n",
    "    dataset_sizes = [len(train_dataset), len(test_dataset), len(val_dataset)]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n",
    "                              shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=NUM_WORKERS, drop_last=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, label_encoder, num_classes, dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predicts, targets, confs):\n",
    "    assert len(predicts.shape) == 1\n",
    "    assert len(confs.shape) == 1\n",
    "    assert len(targets.shape) == 1\n",
    "    assert predicts.shape == confs.shape and confs.shape == targets.shape\n",
    "\n",
    "    _, indices = torch.sort(confs, descending=True)\n",
    "    confs = confs.cpu().numpy()\n",
    "    predicts = predicts[indices].cpu().numpy()\n",
    "    targets = targets[indices].cpu().numpy()\n",
    "\n",
    "    num_correct = 0\n",
    "    for i, (c, p, t) in enumerate(zip(confs, predicts, targets)):\n",
    "        if p == t:\n",
    "            num_correct += 1\n",
    "\n",
    "    return num_correct / len(predicts)\n",
    "            \n",
    "def train(train_loader: Any, model: Any, criterion: Any, optimizer: Any,\n",
    "          epoch: int, lr_scheduler: Any, tensorboard: Any, label_encoder:Any) -> None:\n",
    "    print(f'epoch {epoch}')\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    avg_score = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "    num_steps = min(len(train_loader), MAX_STEPS_PER_EPOCH)\n",
    "\n",
    "    print(f'total batches: {num_steps}')\n",
    "\n",
    "    end = time.time()\n",
    "    lr_str = ''\n",
    "\n",
    "    global_step = (epoch - 1) * len(train_loader)\n",
    "\n",
    "    running_corrects = 0\n",
    "    for i, (input_, target) in enumerate(train_loader):\n",
    "        global_step += 1\n",
    "        if i >= num_steps:\n",
    "            break\n",
    "\n",
    "        output = model(input_.cuda())\n",
    "        loss = criterion(output, target.cuda())\n",
    "\n",
    "        confs, predicts = torch.max(output.detach(), dim=1)\n",
    "        avg_score.update(accuracy(predicts, target, confs))\n",
    "\n",
    "        losses.update(loss.data.item(), input_.size(0))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % LOG_FREQ == 0:\n",
    "            tensorboard.log_scalar(\"train_step_loss\", losses.val, global_step)\n",
    "            tensorboard.log_scalar(\"train_step_accuracy\", avg_score.val, global_step)\n",
    "\n",
    "            print(f'{epoch} [{i}/{num_steps}]\\t'\n",
    "                        f'time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                        f'loss {losses.val:.4f} ({losses.avg:.4f})\\t'\n",
    "                        f'Accuracy {avg_score.val:.4f} ({avg_score.avg:.4f})'\n",
    "                        + lr_str)\n",
    "\n",
    "        if has_time_run_out():\n",
    "            break\n",
    "\n",
    "    avg_epoch_loss = losses.avg\n",
    "    avg_accuracy = avg_score.avg\n",
    "#     avg_epoch_gap = avg_score.avg\n",
    "\n",
    "#     tensorboard.log_scalar(\"train_epoch_loss\", avg_epoch_loss, epoch)\n",
    "#     tensorboard.log_scalar(\"train_epoch_gap\", avg_epoch_gap, epoch)\n",
    "\n",
    "#     torch.save({\n",
    "#         'epoch': epoch,\n",
    "#         'classifier': model.fc,\n",
    "#         'model_state_dict': model.state_dict(),\n",
    "#         'optimizer': optimizer,\n",
    "#         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#         'loss': losses.avg,\n",
    "#         'gap': avg_score.avg,\n",
    "#         'global_step': global_step,\n",
    "#         'label_encoder': label_encoder,\n",
    "#         'resnet_size': RESNET_SIZE,\n",
    "#         'image_size': IMAGE_SIZE\n",
    "#     }, CHECKPOINT_PATH + \"checkpoints_{}\".format(epoch))\n",
    "\n",
    "#     print(f' * average GAP on train {avg_score.avg:.4f}')\n",
    "    \n",
    "def inference(data_loader: Any, model: Any) -> Tuple[torch.Tensor, torch.Tensor,\n",
    "                                                     Optional[torch.Tensor]]:\n",
    "    ''' Returns predictions and targets, if any. '''\n",
    "    model.eval()\n",
    "\n",
    "    activation = nn.Softmax(dim=1)\n",
    "    all_predicts, all_confs, all_targets, all_predicts_gap, all_confs_gap = [], [], [], [], []\n",
    "\n",
    "    print(\"Data loader length\", len(data_loader))\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(data_loader, disable=IN_KERNEL)):\n",
    "            if data_loader.dataset.mode != 'test':\n",
    "                input_, target = data\n",
    "            else:\n",
    "                input_, target = data, None\n",
    "\n",
    "            output = model(input_.cuda())\n",
    "            output = activation(output)\n",
    "\n",
    "            confs, predicts = torch.topk(output, NUM_TOP_PREDICTS)\n",
    "            confs_gap, predicts_gap = torch.max(output.detach(), 1)\n",
    "            all_confs.append(confs)\n",
    "            all_predicts.append(predicts)\n",
    "            all_predicts_gap.append(predicts_gap)\n",
    "            all_confs_gap.append(confs_gap)\n",
    "\n",
    "            if target is not None:\n",
    "                all_targets.append(target)\n",
    "\n",
    "    predicts = torch.cat(all_predicts)\n",
    "    confs = torch.cat(all_confs)\n",
    "    targets = torch.cat(all_targets) if len(all_targets) else None\n",
    "    predicts_gap = torch.cat(all_predicts_gap)\n",
    "    confs_gap = torch.cat(all_confs_gap)\n",
    "\n",
    "    return predicts, confs, targets, predicts_gap, confs_gap\n",
    "\n",
    "def eval(val_loader: Any, train_loader: Any, model: Any, tensorboard: Any, epoch: int) -> np.ndarray:\n",
    "    predicts_gpu, confs_gpu, targets_gpu, predicts_gap_gpu, confs_gap_gpu = inference(val_loader, model)\n",
    "    val_gap = accuracy(predicts_gap_gpu, targets_gpu, confs_gap_gpu)\n",
    "    num_correct = torch.sum(predicts_gap_gpu.cpu() == targets_gpu.cpu())\n",
    "    predicts, confs, targets = predicts_gpu.cpu().numpy(), confs_gpu.cpu().numpy(), targets_gpu.cpu().numpy()\n",
    "\n",
    "\n",
    "    labels = [label_encoder.inverse_transform(pred) for pred in predicts]\n",
    "\n",
    "    assert len(labels) == len(val_loader.dataset.df)\n",
    "\n",
    "    print(f\"Val Accuracy: {val_gap}, Num correct: {num_correct}\")\n",
    "\n",
    "    tensorboard.log_scalar(\"val_num_correct\", num_correct, epoch)\n",
    "    tensorboard.log_scalar(\"val_gap\", val_gap, epoch)\n",
    "\n",
    "    val_df = val_loader.dataset.df\n",
    "    train_df = train_loader.dataset.df\n",
    "    rand_idx = int(random() * len(val_df))\n",
    "\n",
    "    sample_row = val_df.iloc[rand_idx]\n",
    "    sample_target = sample_row[\"category_id\"]\n",
    "    sample_prediction = int(predicts[rand_idx])\n",
    "\n",
    "#     sample_predict_image_name = train_df[train_df[\"category_id\"] == sample_prediction][\"image_id\"].tolist()[0]\n",
    "#     sample_predict_image_path = f\"{IMAGES_DIR}/{sample_predict_image_name}.jpg\"\n",
    "#     sample_correct_label_image_path = f\"{IMAGES_DIR}/{sample_row['image_id'].zfill()}.jpg\"\n",
    "\n",
    "    # images = [image_append_text(sample_predict_image_path,sample_prediction), image_append_text(sample_correct_label_image_path,sample_target)]\n",
    "\n",
    "#     widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "#     total_width = sum(widths)\n",
    "#     max_height = max(heights)\n",
    "\n",
    "#     new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "#     x_offset = 0\n",
    "#     for im in images:\n",
    "#         new_im.paste(im, (x_offset,0))\n",
    "#         x_offset += im.size[0]\n",
    "\n",
    "#     tensorboard.log_image(\"predicted_and_target_image\", np.asarray(new_im), epoch)\n",
    "\n",
    "class Tensorboard:\n",
    "    def __init__(self, logdir):\n",
    "        self.writer = tf.summary.FileWriter(logdir)\n",
    "\n",
    "    def close(self):\n",
    "        self.writer.close()\n",
    "\n",
    "    def log_scalar(self, tag, value, global_step):\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=tag, simple_value=value)\n",
    "        self.writer.add_summary(summary, global_step=global_step)\n",
    "        self.writer.flush()\n",
    "        \n",
    "    def log_histogram(self, tag, values, global_step, bins):\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=tag, histo=hist)\n",
    "        self.writer.add_summary(summary, global_step=global_step)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def log_image(self, tag, img, global_step):\n",
    "        s = io.BytesIO()\n",
    "        Image.fromarray(img).save(s, format='png')\n",
    "\n",
    "        img_summary = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                   height=img.shape[0],\n",
    "                                   width=img.shape[1])\n",
    "\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=tag, image=img_summary)\n",
    "        self.writer.add_summary(summary, global_step=global_step)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def log_plot(self, tag, figure, global_step):\n",
    "        plot_buf = io.BytesIO()\n",
    "        figure.savefig(plot_buf, format='png')\n",
    "        plot_buf.seek(0)\n",
    "        img = Image.open(plot_buf)\n",
    "        img_ar = np.array(img)\n",
    "\n",
    "        img_summary = tf.Summary.Image(encoded_image_string=plot_buf.getvalue(),\n",
    "                                   height=img_ar.shape[0],\n",
    "                                   width=img_ar.shape[1])\n",
    "\n",
    "        summary = tf.Summary()\n",
    "        summary.value.add(tag=tag, image=img_summary)\n",
    "        self.writer.add_summary(summary, global_step=global_step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "classes with at least N samples: 4\n",
      "train_df (7214, 2)\n",
      "train_df after filtering (7214, 2)\n",
      "Train shape after filtering classes:  (5352, 2)\n",
      "Train length: 75 Val length: 2638 Test length: 2639"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "found classes 3\n",
      "creating data loader - train\n",
      "creating data loader - val\n",
      "creating data loader - test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader, label_encoder, n_classes, dataset_sizes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_dir = \"data/\"\n",
    "\n",
    "# model_conv = torchvision.models.resnet50(pretrained=\"imagenet\").cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freeze_layers = True\n",
    "# # n_class = 17143\n",
    "# # n_class = len(glob(data_dir + \"train/*\"))\n",
    "# # Stage-1 Freezing all the layers \n",
    "# if freeze_layers:\n",
    "#     for i, param in model_conv.named_parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# # Since imagenet as 1000 classes , We need to change our last layer according to the number of classes we have,\n",
    "# num_ftrs = model_conv.fc.in_features\n",
    "# model_conv.fc = nn.Linear(num_ftrs, n_classes).cuda()\n",
    "\n",
    "# input_shape = 512\n",
    "# batch_size = 100\n",
    "# mean = [0.5, 0.5, 0.5]\n",
    "# std = [0.5, 0.5, 0.5]\n",
    "# scale = 360\n",
    "# use_parallel = False\n",
    "# use_gpu = True\n",
    "# epochs = 100\n",
    "\n",
    "# data_transforms = {\n",
    "#         'train': transforms.Compose([\n",
    "#         transforms.Resize(scale),\n",
    "#         transforms.RandomResizedCrop(input_shape),\n",
    "#         transforms.RandomHorizontalFlip(),\n",
    "# #         transforms.RandomVerticalFlip(),\n",
    "#         transforms.RandomRotation(degrees=90),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)]),\n",
    "#         'val': transforms.Compose([\n",
    "#         transforms.Resize(scale),\n",
    "#         transforms.CenterCrop(input_shape),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean, std)]),\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "#                                       data_transforms[x]) for x in ['train', 'val']}\n",
    "# dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "#                                          shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "\n",
    "# dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "# class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "# dataloaders = {\n",
    "#     \"train\": train_loader,\n",
    "#     \"val\": val_loader,\n",
    "#     \"test\": test_loader\n",
    "# }\n",
    "\n",
    "# class_names = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if use_parallel:\n",
    "#     print(\"[Using all the available GPUs]\")\n",
    "#     model_conv = nn.DataParallel(model_conv, device_ids=[0, 1])\n",
    "\n",
    "# print(\"[Using CrossEntropyLoss...]\")\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# print(\"[Using small learning rate with momentum...]\")\n",
    "# optimizer_conv = optim.SGD(list(filter(lambda p: p.requires_grad, model_conv.parameters())), lr=0.001, momentum=0.9)\n",
    "\n",
    "# # model_conv, optimizer_conv = amp.initialize(model_conv, optimizer_conv, opt_level=\"O1\")\n",
    "# print(\"[Creating Learning rate scheduler...]\")\n",
    "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(\"[Training the model begun ....]\")\n",
    "# model_ft = utils.train_model(model_conv, dataloaders, dataset_sizes, criterion, optimizer_conv, exp_lr_scheduler, use_gpu,\n",
    "#                      num_epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data...\n",
      "classes with at least N samples: 4\n",
      "train_df (7214, 2)\n",
      "train_df after filtering (7214, 2)\n",
      "Train shape after filtering classes:  (5352, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/daniel/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train length: 75 Val length: 2638 Test length: 2639\n",
      "found classes 3\n",
      "creating data loader - train\n",
      "creating data loader - val\n",
      "creating data loader - test\n",
      "Training...\n",
      "--------------------------------------------------\n",
      "epoch 1\n",
      "total batches: 3\n",
      "1 [0/3]\ttime 0.360 (0.360)\tloss 1.3200 (1.3200)\tAccuracy 0.2000 (0.2000)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c193f6dca1384953bec85921171b37dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7539802880970432, Num correct: 1989\n",
      "--------------------------------------------------\n",
      "epoch 2\n",
      "total batches: 3\n",
      "2 [0/3]\ttime 0.346 (0.346)\tloss 0.4835 (0.4835)\tAccuracy 0.7200 (0.7200)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e493f8a95bdf45f59fb312d3aaf96574"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.525018953752843, Num correct: 1385\n",
      "--------------------------------------------------\n",
      "epoch 3\n",
      "total batches: 3\n",
      "3 [0/3]\ttime 0.345 (0.345)\tloss 0.1818 (0.1818)\tAccuracy 0.8800 (0.8800)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4263e06c984ba38bb4f34dc12d48c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.5856709628506445, Num correct: 1545\n",
      "--------------------------------------------------\n",
      "epoch 4\n",
      "total batches: 3\n",
      "4 [0/3]\ttime 0.342 (0.342)\tloss 1.2428 (1.2428)\tAccuracy 0.6400 (0.6400)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "facf1718d3564f5db37648bad0ba0fda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.48142532221379836, Num correct: 1270\n",
      "--------------------------------------------------\n",
      "epoch 5\n",
      "total batches: 3\n",
      "5 [0/3]\ttime 0.343 (0.343)\tloss 0.5899 (0.5899)\tAccuracy 0.6800 (0.6800)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2b00bf12f846348c2470b75ef413e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.46739954510993176, Num correct: 1233\n",
      "--------------------------------------------------\n",
      "epoch 6\n",
      "total batches: 3\n",
      "6 [0/3]\ttime 0.341 (0.341)\tloss 0.6689 (0.6689)\tAccuracy 0.7600 (0.7600)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb99aa2a0b2481c8399cdb92ce671cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.643669446550417, Num correct: 1698\n",
      "--------------------------------------------------\n",
      "epoch 7\n",
      "total batches: 3\n",
      "7 [0/3]\ttime 0.398 (0.398)\tloss 0.4650 (0.4650)\tAccuracy 0.8400 (0.8400)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "644bd66de12e462991c710324b661673"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.675890826383624, Num correct: 1783\n",
      "--------------------------------------------------\n",
      "epoch 8\n",
      "total batches: 3\n",
      "8 [0/3]\ttime 0.344 (0.344)\tloss 0.3909 (0.3909)\tAccuracy 0.8400 (0.8400)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b5be3db95374c18addd4f45b52236ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.6830932524639879, Num correct: 1802\n",
      "--------------------------------------------------\n",
      "epoch 9\n",
      "total batches: 3\n",
      "9 [0/3]\ttime 0.354 (0.354)\tloss 0.2868 (0.2868)\tAccuracy 0.8400 (0.8400)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e7e3a3f96654509a42b9a7f10a0b010"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.6884003032600455, Num correct: 1816\n",
      "--------------------------------------------------\n",
      "epoch 10\n",
      "total batches: 3\n",
      "10 [0/3]\ttime 0.346 (0.346)\tloss 0.0768 (0.0768)\tAccuracy 1.0000 (1.0000)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b847c07779244f3977f1256b5ad2530"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.721379833206975, Num correct: 1903\n",
      "--------------------------------------------------\n",
      "epoch 11\n",
      "total batches: 3\n",
      "11 [0/3]\ttime 0.355 (0.355)\tloss 0.1811 (0.1811)\tAccuracy 1.0000 (1.0000)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac6011da9dcf40159c8a00e266e06722"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7426080363912054, Num correct: 1959\n",
      "--------------------------------------------------\n",
      "epoch 12\n",
      "total batches: 3\n",
      "12 [0/3]\ttime 0.345 (0.345)\tloss 0.1499 (0.1499)\tAccuracy 0.9200 (0.9200)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74096d336294b68ad8758ada14a22db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.768385140257771, Num correct: 2027\n",
      "--------------------------------------------------\n",
      "epoch 13\n",
      "total batches: 3\n",
      "13 [0/3]\ttime 0.350 (0.350)\tloss 0.2471 (0.2471)\tAccuracy 0.9200 (0.9200)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2c84b607f7a4e818064040c0843f8dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.775587566338135, Num correct: 2046\n",
      "--------------------------------------------------\n",
      "epoch 14\n",
      "total batches: 3\n",
      "14 [0/3]\ttime 0.353 (0.353)\tloss 0.1604 (0.1604)\tAccuracy 0.9600 (0.9600)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a4e54ed68047d8a846657ac7470bfd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7767247915087188, Num correct: 2049\n",
      "--------------------------------------------------\n",
      "epoch 15\n",
      "total batches: 3\n",
      "15 [0/3]\ttime 0.368 (0.368)\tloss 0.1975 (0.1975)\tAccuracy 0.9600 (0.9600)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0604af7786124d968306a6480a89991c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7763457164518575, Num correct: 2048\n",
      "--------------------------------------------------\n",
      "epoch 16\n",
      "total batches: 3\n",
      "16 [0/3]\ttime 0.345 (0.345)\tloss 0.1180 (0.1180)\tAccuracy 0.9600 (0.9600)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4777fe82eaac409ca74f1713773be00d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7808946171341926, Num correct: 2060\n",
      "--------------------------------------------------\n",
      "epoch 17\n",
      "total batches: 3\n",
      "17 [0/3]\ttime 0.358 (0.358)\tloss 0.2014 (0.2014)\tAccuracy 0.9600 (0.9600)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c328ee7b283e4e9bb2bab705adf070d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7824109173616376, Num correct: 2064\n",
      "--------------------------------------------------\n",
      "epoch 18\n",
      "total batches: 3\n",
      "18 [0/3]\ttime 0.343 (0.343)\tloss 0.1298 (0.1298)\tAccuracy 0.9200 (0.9200)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2539b194ab1d46b48b31b90feeadf985"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.785822592873389, Num correct: 2073\n",
      "--------------------------------------------------\n",
      "epoch 19\n",
      "total batches: 3\n",
      "19 [0/3]\ttime 0.351 (0.351)\tloss 0.1117 (0.1117)\tAccuracy 0.9600 (0.9600)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e0b9fc52d94a4cac4b7c8b3e1435a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7884761182714177, Num correct: 2080\n",
      "--------------------------------------------------\n",
      "epoch 20\n",
      "total batches: 3\n",
      "20 [0/3]\ttime 0.336 (0.336)\tloss 0.2662 (0.2662)\tAccuracy 0.8400 (0.8400)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fd71cefe464b43b782091c14b5a30a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val Accuracy: 0.7907505686125853, Num correct: 2086\n",
      "--------------------------------------------------\n",
      "epoch 21\n",
      "total batches: 3\n",
      "21 [0/3]\ttime 0.347 (0.347)\tloss 0.1061 (0.1061)\tAccuracy 0.9600 (0.9600)\n",
      "Data loader length 106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed44753129e24bf48e88a1c6833a0a57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CHECKPOINT_PATH = \"checkpoints\"\n",
    "CHECKPOINT_NAME = None\n",
    "tensorboard = Tensorboard(CHECKPOINT_PATH + \"logdir\")\n",
    "epoch = 1\n",
    "\n",
    "if CHECKPOINT_NAME != None:\n",
    "    checkpoint = torch.load(CHECKPOINT_PATH + CHECKPOINT_NAME)\n",
    "    train_loader, val_loader, test_loader, label_encoder, num_classes,_ = load_data(checkpoint)\n",
    "else:\n",
    "    train_loader, val_loader, test_loader, label_encoder, num_classes, _ = load_data()\n",
    "\n",
    "if RESNET_SIZE == 50:\n",
    "    model = torchvision.models.resnet50(pretrained=True)\n",
    "elif RESNET_SIZE == 101:\n",
    "    model = torchvision.models.resnet101(pretrained=True)\n",
    "elif RESNET_SIZE == 18:\n",
    "    model = torchvision.models.resnet18(pretrained=True)\n",
    "else:\n",
    "    raise ValueError(\"Invalid resnet size: \", RESNET_SIZE)\n",
    "model.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "model.cuda()\n",
    "\n",
    "if CHECKPOINT_NAME != None:\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    epoch = int(checkpoint[\"epoch\"]) + 1\n",
    "    global_step = int(checkpoint[\"global_step\"])\n",
    "\n",
    "global_start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "if not PREDICT_ONLY:\n",
    "    print(\"Training...\")\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    if CHECKPOINT_NAME != None:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    # model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP,\n",
    "                                                   gamma=LR_FACTOR)\n",
    "\n",
    "    if USE_PARALLEL:\n",
    "        print(\"[Using all the available GPUs]\")\n",
    "        model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "    for epoch in range(epoch, NUM_EPOCHS + 1):\n",
    "        print('-' * 50)\n",
    "        train(train_loader, model, criterion, optimizer, epoch, lr_scheduler, tensorboard, label_encoder)\n",
    "        eval(val_loader, train_loader, model, tensorboard, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if has_time_run_out():\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
